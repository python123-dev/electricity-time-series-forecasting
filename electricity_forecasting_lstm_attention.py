# -*- coding: utf-8 -*-
"""power_DL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mbat4U-kPArOufFD_qJFl-P3ieMfJTBU
"""

import pandas as pd

df=pd.read_csv('/content/household_power_consumption.csv')
df.head()

df.info()

import pandas as pd

# Combine Date and Time into a single datetime column
df['datetime'] = pd.to_datetime(
    df['Date'] + ' ' + df['Time'],
    format='%d/%m/%Y %H:%M:%S'
)

# Set datetime as index
df.set_index('datetime', inplace=True)

# Drop old Date and Time columns
df.drop(columns=['Date', 'Time'], inplace=True)

# Verify
df.head()

# Convert all columns to numeric
for col in df.columns:
    df[col] = pd.to_numeric(df[col], errors='coerce')

# Check result
df.info()

#looking for missing values

df.isna().sum()

df.describe()

#How many missing rows are there (percentage)?

missing_counts = df.isna().sum()
missing_percentage = (missing_counts / len(df)) * 100

print(missing_percentage)

"""“Given the large size of the dataset and the small proportion of missing values (<1%), rows containing missing values were removed. This approach avoids introducing imputation bias while preserving the temporal structure and statistical properties of the data.”"""

# Drop rows with any missing values
df_clean = df.dropna()

print("Before:", df.shape)
print("After :", df_clean.shape)

"""“The original dataset contains minute-level measurements, which are highly noisy and result in very long sequences. To improve model stability, interpretability, and computational efficiency, the data was resampled to hourly resolution. Mean aggregation was used for continuous electrical measurements, while summation was applied to sub-metered energy consumption variables.”"""

# Resample to hourly data
df_hourly = df_clean.resample('h').agg({  #'h' for hours
    'Global_active_power': 'mean',
    'Global_reactive_power': 'mean',
    'Voltage': 'mean',
    'Global_intensity': 'mean',
    'Sub_metering_1': 'sum',
    'Sub_metering_2': 'sum',
    'Sub_metering_3': 'sum'
})

# Drop any remaining NaNs after resampling
df_hourly.dropna(inplace=True)

# Check result
df_hourly.head()

print(df_hourly.shape)
print(df_hourly.index.min(), df_hourly.index.max())
print(df_hourly.isna().sum())

import matplotlib.pyplot as plt

df_hourly['Global_active_power'].plot(figsize=(12,4))
plt.title("Hourly Global Active Power Consumption")
plt.show()

"""The forecasting task is formulated as a multivariate sequence-to-sequence problem, where past observations of household electricity consumption and related electrical variables are used to predict future values of global active power. This setup allows the model to leverage inter-variable dependencies while maintaining a clear and interpretable forecasting objective."""

# total length
n = len(df_hourly)

train_end = int(n * 0.7)
val_end   = int(n * 0.85)

train_data = df_hourly.iloc[:train_end]
val_data   = df_hourly.iloc[train_end:val_end]
test_data  = df_hourly.iloc[val_end:]

print("Train:", train_data.shape)
print("Val  :", val_data.shape)
print("Test :", test_data.shape)

#scaling
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

train_scaled = scaler.fit_transform(train_data)
val_scaled   = scaler.transform(val_data)
test_scaled  = scaler.transform(test_data)

feature_cols = [
    'Global_active_power',
    'Global_reactive_power',
    'Voltage',
    'Global_intensity',
    'Sub_metering_1',
    'Sub_metering_2',
    'Sub_metering_3'
]

train_scaled = pd.DataFrame(
    train_scaled,
    columns=feature_cols,
    index=train_data.index
)

val_scaled = pd.DataFrame(
    val_scaled,
    columns=feature_cols,
    index=val_data.index
)

test_scaled = pd.DataFrame(
    test_scaled,
    columns=feature_cols,
    index=test_data.index
)

train_scaled.head()

# TEST TRAIN

import numpy as np

# window sizes
INPUT_WINDOW = 24      # past 24 hours
OUTPUT_WINDOW = 6      # predict next 6 hours
TARGET_COL = 'Global_active_power'


def create_sequences(data, target_col, input_window, output_window):
    X, y = [], []

    target_idx = data.columns.get_loc(target_col)
    values = data.values

    for i in range(len(values) - input_window - output_window):
        X.append(values[i:i + input_window])
        y.append(values[i + input_window:i + input_window + output_window, target_idx])

    return np.array(X), np.array(y)


# create sequences
X_train, y_train = create_sequences(train_scaled, TARGET_COL, INPUT_WINDOW, OUTPUT_WINDOW)
X_val, y_val     = create_sequences(val_scaled, TARGET_COL, INPUT_WINDOW, OUTPUT_WINDOW)
X_test, y_test   = create_sequences(test_scaled, TARGET_COL, INPUT_WINDOW, OUTPUT_WINDOW)

# check shapes
print("X_train:", X_train.shape, "y_train:", y_train.shape)
print("X_val  :", X_val.shape, "y_val  :", y_val.shape)
print("X_test :", X_test.shape, "y_test :", y_test.shape)

#LSTM Starts

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
import matplotlib.pyplot as plt

# build model
model = Sequential([
    LSTM(
        units=64,
        activation='tanh',
        input_shape=(X_train.shape[1], X_train.shape[2])
    ),
    Dense(OUTPUT_WINDOW)
])

# compile
model.compile(
    optimizer='adam',
    loss='mse'
)

# model summary (important for report)
model.summary()

# train
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=20,
    batch_size=64,
    verbose=1
)

# plot loss
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('MSE')
plt.legend()
plt.show()

"""“The LSTM model demonstrates strong predictive capability, closely tracking real electricity consumption trends. While peak values are slightly smoothed—an expected behavior in sequence models—the model successfully captures temporal patterns and fluctuations. The achieved RMSE of 0.56 kW indicates reliable performance for short-term load forecasting.”"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error

# 1️⃣ Predict on test set
y_pred_scaled = model.predict(X_test)

print("Scaled prediction shape:", y_pred_scaled.shape)

# 2️⃣ Rebuild full feature matrix for inverse scaling
# Create empty array for 7 features
dummy_pred = np.zeros((y_pred_scaled.shape[0], 7))
dummy_true = np.zeros((y_test.shape[0], 7))

# Assume target column index = 0 (Global_active_power)
dummy_pred[:, 0] = y_pred_scaled[:, 0]
dummy_true[:, 0] = y_test[:, 0]

# 3️⃣ Inverse scaling
y_pred_inv = scaler.inverse_transform(dummy_pred)[:, 0]
y_true_inv = scaler.inverse_transform(dummy_true)[:, 0]

# 4️⃣ RMSE in real units
rmse = np.sqrt(mean_squared_error(y_true_inv, y_pred_inv))
print(f"Test RMSE (kW): {rmse:.3f}")

# 5️⃣ Plot actual vs predicted
plt.figure(figsize=(14,5))
plt.plot(y_true_inv[:300], label="Actual Power Consumption")
plt.plot(y_pred_inv[:300], label="Predicted Power Consumption")
plt.title("Electricity Consumption Prediction (Test Set)")
plt.xlabel("Time Steps")
plt.ylabel("Global Active Power (kW)")
plt.legend()
plt.grid(True)
plt.show()

#helper

import numpy as np

def inverse_transform_target(preds, scaler, target_index, num_features):
    """
    preds: (samples, horizon)
    returns: inverse-scaled preds with same shape
    """
    temp = np.zeros((preds.shape[0] * preds.shape[1], num_features))
    temp[:, target_index] = preds.flatten()
    temp_inv = scaler.inverse_transform(temp)
    return temp_inv[:, target_index].reshape(preds.shape)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import mean_squared_error


dense_model = Sequential([
    Flatten(input_shape=(X_train.shape[1], X_train.shape[2])),
    Dense(64, activation='relu'),
    Dense(y_train.shape[1])  # horizon = 6 or 12
])

dense_model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='mse'
)

dense_model.summary()


history_dense = dense_model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=20,
    batch_size=64,
    verbose=1
)

dense_preds = dense_model.predict(X_test)
print("Dense prediction shape:", dense_preds.shape)

target_col_index = feature_cols.index("Global_active_power")

dense_preds_inv = inverse_transform_target(
    dense_preds,
    scaler,
    target_col_index,
    len(feature_cols)
)

dense_rmse = np.sqrt(mean_squared_error(
    y_test.flatten(),
    dense_preds_inv.flatten()
))

print(f"Dense Model RMSE (kW): {dense_rmse:.3f}")

"""The LSTM model significantly outperformed the Dense baseline model.
The Dense model achieved an RMSE of 1.106 kW, while the LSTM reduced the error to 0.562 kW.
This demonstrates the importance of temporal dependency modeling in electricity consumption forecasting, as LSTM networks are capable of learning sequential patterns and long-term dependencies that traditional feed-forward models cannot capture.
"""

print(f"LSTM RMSE  (kW): {rmse:.3f}")
print(f"Dense RMSE (kW): {dense_rmse:.3f}")

"""Why Attention?
LSTM processes all timesteps equally.
Attention learns which past timesteps matter more for prediction.

In electricity demand:

Recent hours → high importance

Certain past cycles → medium importance

Old data → low importance

This directly satisfies your project requirement.
"""

# adding attention

import tensorflow as tf
from tensorflow.keras.layers import Layer

class AttentionLayer(Layer):
    def __init__(self):
        super(AttentionLayer, self).__init__()

    def build(self, input_shape):
        self.W = self.add_weight(
            name="att_weight",
            shape=(input_shape[-1], 1),
            initializer="normal"
        )
        self.b = self.add_weight(
            name="att_bias",
            shape=(input_shape[1], 1),
            initializer="zeros"
        )

    def call(self, x):
        # x shape: (batch, timesteps, features)
        e = tf.keras.backend.tanh(
            tf.keras.backend.dot(x, self.W) + self.b
        )
        a = tf.keras.backend.softmax(e, axis=1)
        output = x * a
        return tf.keras.backend.sum(output, axis=1), a

#LSTM + Attention

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense
from tensorflow.keras.optimizers import Adam

# Input
inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))

# LSTM
lstm_out = LSTM(
    64,
    return_sequences=True
)(inputs)

# Attention
context_vector, attention_weights = AttentionLayer()(lstm_out)

# Output layer
outputs = Dense(y_train.shape[1])(context_vector)

# Model
att_model = Model(inputs=inputs, outputs=outputs)

att_model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='mse'
)

att_model.summary()

#train model

history_att = att_model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=20,
    batch_size=64,
    verbose=1
)

att_preds = att_model.predict(X_test)

# Create dummy array
att_preds_full = np.zeros((att_preds.shape[0], X_test.shape[2]))

# Put predictions in the first columns (target position)
att_preds_full[:, :y_test.shape[1]] = att_preds

# Inverse transform
att_preds_inv = scaler.inverse_transform(att_preds_full)[:, :y_test.shape[1]]

att_rmse_scaled = np.sqrt(
    mean_squared_error(y_test.flatten(), att_preds.flatten())
)

print(f"LSTM + Attention RMSE (scaled): {att_rmse_scaled:.3f}")

print(f"LSTM RMSE  (kW): {rmse:.3f}")
print(f"Dense RMSE (kW): {dense_rmse:.3f}")
print(f"LSTM + Attention RMSE (kW): {att_rmse_scaled:.3f}")

# Create attention extractor model
attention_extractor = Model(
    inputs=att_model.input,
    outputs=attention_weights
)

att_weights = attention_extractor.predict(X_test)

"""“The attention weight visualization reveals that the model assigns significantly higher importance to the most recent historical time steps, particularly the last 5–6 hours prior to prediction. This indicates that short-term temporal dependencies play a dominant role in forecasting future electricity demand. Earlier time steps contribute moderately, likely capturing broader usage patterns, while mid-range historical values have minimal influence. This behavior aligns well with domain expectations and validates the effectiveness of the attention mechanism in identifying relevant temporal context.”"""

import matplotlib.pyplot as plt

# Average attention across samples
mean_attention = att_weights.mean(axis=0).squeeze()

plt.figure(figsize=(10,4))
plt.plot(mean_attention)
plt.title("Average Attention Weights Across Time Steps")
plt.xlabel("Time Step (Past 24 Hours)")
plt.ylabel("Attention Weight")
plt.grid(True)
plt.show()